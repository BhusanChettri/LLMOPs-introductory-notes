{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFB85CMAOaFjTfvSiUENXJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# L2: Data Exploration for Tuning a Foundation Model"],"metadata":{"id":"_j2nnhwI88Ps"}},{"cell_type":"markdown","source":["**Project environment setup:**\n","\n","- Load credentials and relevant Python Libraries\n","- If you were running this notebook locally, you would first install Vertex AI. In this classroom, this is already installed.\n","\n","```\n","!pip install google-cloud-aiplatform\n","```\n","- You can download the `requirements.txt` for this course from the workspace of this lab. `File --> Open...`"],"metadata":{"id":"JK1Ki3VY8_4H"}},{"cell_type":"code","source":["from utils import authenticate\n","credentials, PROJECT_ID = authenticate()"],"metadata":{"id":"BCxzlPHJ88tr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["REGION = \"us-central1\""],"metadata":{"id":"lfULDEYa946a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Import the [Vertex AI](https://cloud.google.com/vertex-ai) SDK.\n","- The library helps to interact with the Vertex AI services in the cloud.\n","- Initialize it."],"metadata":{"id":"5-MeRqhR96zB"}},{"cell_type":"code","source":["import vertexai"],"metadata":{"id":"suy_fXAv9_N_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vertexai.init(project = PROJECT_ID,\n","              location = REGION,\n","              credentials = credentials)"],"metadata":{"id":"gLCp5SkL-CDU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Import [BigQuery](https://cloud.google.com/bigquery) to use as your data warehouse.\n","- Initialize the client to start interacting with the data warehouse, send SQL and retrieve data into the notebook."],"metadata":{"id":"kPnagVZ1-HyC"}},{"cell_type":"code","source":["from google.cloud import bigquery"],"metadata":{"id":"QTre1Lf1-ECs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bq_client = bigquery.Client(project=PROJECT_ID,\n","                            credentials = credentials)"],"metadata":{"id":"ldQyfnUt-QRJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stack Overflow Public Dataset"],"metadata":{"id":"jBjPNV_L-UiX"}},{"cell_type":"markdown","source":["- You will use [Stack Overflow Data](https://cloud.google.com/blog/topics/public-datasets/google-bigquery-public-datasets-now-include-stack-overflow-q-a) on BigQuery Public Datasets.\n","- The datasets include questions, answers and metadata related to Stack Overflow questions. Within this dataset, there are tables with data.\n","- Create a SQL query."],"metadata":{"id":"fEnsDE_u-XXf"}},{"cell_type":"code","source":["QUERY_TABLES = \"\"\"\n","SELECT\n","  table_name\n","FROM\n","  `bigquery-public-data.stackoverflow.INFORMATION_SCHEMA.TABLES`\n","\"\"\""],"metadata":{"id":"fj9MkGO8-TaU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- The query is asking to retrieve `table_name` of all the `TABLES`\n","- Use the client to send your SQL and retrieve the data (tables names)."],"metadata":{"id":"bvAH7w2--dal"}},{"cell_type":"code","source":["query_job = bq_client.query(QUERY_TABLES)"],"metadata":{"id":"d7YkKQSz-fwf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for row in query_job:\n","    for value in row.values():\n","        print(value)"],"metadata":{"id":"uTY4BuWh-ih0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Retrieval"],"metadata":{"id":"EiMCJIKj-nia"}},{"cell_type":"markdown","source":["- You'll fetch some data from the data warehouse and store it in Pandas dataframe for visualization.\n","- Select all columns from  `posts_questions` and put the `LIMIT` as 3."],"metadata":{"id":"uzcqFd4q-qpw"}},{"cell_type":"code","source":["INSPECT_QUERY = \"\"\"\n","SELECT\n","    *\n","FROM\n","    `bigquery-public-data.stackoverflow.posts_questions`\n","LIMIT 3\n","\"\"\""],"metadata":{"id":"JZW-m8DI-k8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"wpDXofIe-uyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_job = bq_client.query(INSPECT_QUERY)"],"metadata":{"id":"4ALXoY_v-wp9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Take the results of the query `-->` create an arrow table (which is part of [Apache Framework](https://arrow.apache.org/docs/index.html)) `-->` which goes into a Pandas dataframe.\n","- This allows for data to be in a format which is easier to read and explore with Pandas."],"metadata":{"id":"3RPPeO-W-2PS"}},{"cell_type":"code","source":["stack_overflow_df = query_job\\\n","    .result()\\\n","    .to_arrow()\\\n","    .to_pandas()\n","stack_overflow_df.head()"],"metadata":{"id":"UM3dHJNu-2zT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dealing with Large Datasets\n","\n","- Large datasets for LLMs often don't fit into memory.\n","- Select all of the columns and rows of the table `posts_questions`."],"metadata":{"id":"tBRcl_ty-7N8"}},{"cell_type":"code","source":["QUERY_ALL = \"\"\"\n","SELECT\n","    *\n","FROM\n","    `bigquery-public-data.stackoverflow.posts_questions` q\n","\"\"\""],"metadata":{"id":"efxh_oHg-7oL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_job = bq_client.query(QUERY_ALL)"],"metadata":{"id":"CHiDJqme--GT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["try:\n","    stack_overflow_df = query_job\\\n","    .result()\\\n","    .to_arrow()\\\n","    .to_pandas()\n","except Exception as e:\n","    print('The DataFrame is too large to load into memory.', e)"],"metadata":{"id":"zq9scEOM_AEk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** The data is too large to return, as it is not fitting into memory."],"metadata":{"id":"TFiPf2gY_E11"}},{"cell_type":"markdown","source":["#### Joining Tables and Query Optimization\n","\n","- When working with (large) data, query optimizing is needed in order to save time and resources.\n","- Select questions as `input_text` (column 1), answers as `output_text` (column 2).\n","- Take the questions from `posts_questions` and answers from `posts_answers`.\n","- Join the questions and their corresponding accepted answers based on their same `unique ID`.\n","- Making sure the question is about `Python`, and that it `has an answer`. And the date the question was posted is on or after `2020-01-01`\n","- Limit as 10,000"],"metadata":{"id":"lUqKzqsk_HsU"}},{"cell_type":"code","source":["QUERY = \"\"\"\n","SELECT\n","    CONCAT(q.title, q.body) as input_text,\n","    a.body AS output_text\n","FROM\n","    `bigquery-public-data.stackoverflow.posts_questions` q\n","JOIN\n","    `bigquery-public-data.stackoverflow.posts_answers` a\n","ON\n","    q.accepted_answer_id = a.id\n","WHERE\n","    q.accepted_answer_id IS NOT NULL AND\n","    REGEXP_CONTAINS(q.tags, \"python\") AND\n","    a.creation_date >= \"2020-01-01\"\n","LIMIT\n","    10000\n","\"\"\""],"metadata":{"id":"NwkMkVxA_CMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["query_job = bq_client.query(QUERY)"],"metadata":{"id":"fiQiSyri_L1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### this may take some seconds to run\n","stack_overflow_df = query_job.result()\\\n","                        .to_arrow()\\\n","                        .to_pandas()\n","\n","stack_overflow_df.head(2)"],"metadata":{"id":"8UUR5Gvy_N1V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Adding Instructions\n","\n","- Instructions for LLMs have been shown to improve\n","model performance and generalization to unseen tasks [(Google, 2022)](https://arxiv.org/pdf/2210.11416.pdf).\n","- Wihtout the instruction, it is only question and answer. Model might not understand what to do.\n","- With the instructions, the model gets a guideline as to what task to perform."],"metadata":{"id":"wpA9C43v_R8V"}},{"cell_type":"code","source":["INSTRUCTION_TEMPLATE = f\"\"\"\\\n","Please answer the following Stackoverflow question on Python. \\\n","Answer it like you are a developer answering Stackoverflow questions.\n","\n","Stackoverflow question:\n","\"\"\""],"metadata":{"id":"Qn6YkexK_SZQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- A new column will combine `INSTRUCTION_TEMPLATE` and the question `input_text`.\n","- This avoids overwritting of any existing column which might be needed."],"metadata":{"id":"4GIs42SP_YVZ"}},{"cell_type":"code","source":["stack_overflow_df['input_text_instruct'] = INSTRUCTION_TEMPLATE + ' '\\\n","    + stack_overflow_df['input_text']"],"metadata":{"id":"8m-Q7tu9_ay0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset for Tuning\n","\n","- Divide the data into a training and evaluation. By default, 80/20 split is used.\n","- This (80/20 split) allows for more data to be used for tuning. The evaluation split is used as unseen data during tuning to evaluate performance.\n","- The `random_state` parameter is used to ensure random sampling for a fair comparison."],"metadata":{"id":"hsIsgIbc_emE"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"4-Aw6gYb_fBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train, evaluation = train_test_split(\n","    stack_overflow_df,\n","    ### test_size=0.2 means 20% for evaluation\n","    ### which then makes train set to be of 80%\n","    test_size=0.2,\n","    random_state=42\n",")"],"metadata":{"id":"2JLShhmu_jiJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Different Datasets and Flow\n","\n","- Versioning data is important.\n","- It allows for reproducibility, traceability, and maintainability of machine learning models.\n","- Get the timestamp."],"metadata":{"id":"mr-GqR1J_oyq"}},{"cell_type":"code","source":["import datetime"],"metadata":{"id":"fxuK7-Y3_vDt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["date = datetime.datetime.now().strftime(\"%H:%d:%m:%Y\")"],"metadata":{"id":"Mp8mLHMW_xQj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Generate a `jsonl` file.\n","- Name it as `tune_data_stack_overflow_python_qa-{date}`"],"metadata":{"id":"jCV0SDm7_zrY"}},{"cell_type":"code","source":["cols = ['input_text_instruct','output_text']\n","tune_jsonl = train[cols].to_json(orient=\"records\", lines=True)"],"metadata":{"id":"z4uy1kVC_0OL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_data_filename = f\"tune_data_stack_overflow_\\\n","                            python_qa-{date}.jsonl\""],"metadata":{"id":"c0dC1pG__4Gk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(training_data_filename, \"w\") as f:\n","    f.write(tune_jsonl)"],"metadata":{"id":"E4bRzgrk_6TO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Try it Yourself! - Evaluation Set\n","\n","The code above generted a `jsonl` file for the `train` set. Now, its time for you to make the `evaluation` set, which you can name as `tune_eval_data_stack_overflow_python_qa-{date}.jsonl`. The code for that is also provided to you in the drop down below, but we encourage you to try it yourself first before you look at it.\n","\n","<details>\n","  <summary><font size=\"2\" color=\"darkgreen\"><b>Code for Evaluation Set (Click to expand)</b></font></summary>\n","    \n","```python\n","\n","cols = ['input_text_instruct','output_text']\n","### you need to use the \"evaluation\" set now\n","tune_jsonl = evaluation[cols].to_json(orient=\"records\", lines=True)\n","\n","### change the file name\n","### use \"tune_eval_data_stack_overflow_python_qa-{date}.jsonl\"\n","evaluation_data_filename = f\"tune_eval_data_stack_overflow_\\\n","                            python_qa-{date}.jsonl\"\n","\n","### write the file\n","with open(evaluation_data_filename, \"w\") as f:\n","    f.write(tune_jsonl)\n","\n","```"],"metadata":{"id":"6diLeLsi_-h3"}},{"cell_type":"code","source":[],"metadata":{"id":"pRkt_AGC__DL"},"execution_count":null,"outputs":[]}]}